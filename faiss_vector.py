# -*- coding: utf-8 -*-
"""Faiss.vector

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yuiTNayccr9NFCCPqFgQXrHc5CWuLNXt
"""

!pip install faiss-cpu

import faiss
print("FAISS version:", faiss.__version__)

# Load CSV
df = pd.read_csv("master_dataset_with_embeddings (1).csv")

# See the column names
print(df.columns)

df = df.rename(columns={
    'VideoID': 'video_id',   # example
    'Title': 'title',
    'Transcript': 'transcript',
    'ChannelTitle': 'channel_title',
    'Views': 'view_count',
    'Duration': 'duration'
})

# Only combine title + transcript for embeddings
df['combined_text'] = df['title'].astype(str) + " " + df['transcript'].astype(str)

# Generate embeddings as usual
# ... rest of FAISS code

# -----------------------------
# Step 0: Install dependencies
# -----------------------------
!pip install --quiet pandas sentence-transformers faiss-cpu

# -----------------------------
# Step 1: Import libraries
# -----------------------------
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import pickle
from google.colab import files

# -----------------------------
# Step 2: Upload CSV file
# -----------------------------
print("Upload your CSV file (must have at least 'title' and 'transcript')")
uploaded = files.upload()
csv_file = list(uploaded.keys())[0]

df = pd.read_csv(csv_file)
print("Columns in your CSV:", df.columns.tolist())

# -----------------------------
# Step 3: Handle missing columns
# -----------------------------
# Required for embeddings
if 'title' not in df.columns or 'transcript' not in df.columns:
    raise ValueError("Your CSV must have at least 'title' and 'transcript' columns!")

# Optional metadata columns
optional_cols = ['video_id', 'channel_title', 'view_count', 'duration']
for col in optional_cols:
    if col not in df.columns:
        print(f"Warning: Column '{col}' not found. It will be skipped in metadata.")

# -----------------------------
# Step 4: Combine title + transcript
# -----------------------------
df['combined_text'] = df['title'].astype(str) + " " + df['transcript'].astype(str)

# -----------------------------
# Step 5: Load embedding model
# -----------------------------
print("Loading embedding model...")
model = SentenceTransformer('all-MiniLM-L6-v2')

# -----------------------------
# Step 6: Generate embeddings
# -----------------------------
print("Generating embeddings...")
embeddings = model.encode(df['combined_text'].tolist(), show_progress_bar=True)
embedding_matrix = np.array(embeddings).astype('float32')

# -----------------------------
# Step 7: Create FAISS index
# -----------------------------
dimension = embedding_matrix.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embedding_matrix)
print(f"FAISS index created with {index.ntotal} vectors.")

# -----------------------------
# Step 8: Save FAISS index
# -----------------------------
faiss_index_file = "faiss_video_index.index"
faiss.write_index(index, faiss_index_file)
print(f"FAISS index saved to {faiss_index_file}")

# -----------------------------
# Step 9: Save metadata
# -----------------------------
metadata = []
for i, row in df.iterrows():
    entry = {}
    for col in ['video_id', 'title', 'transcript', 'channel_title', 'view_count', 'duration']:
        if col in df.columns:
            entry[col] = row[col]
    metadata.append(entry)

metadata_file = "faiss_video_metadata.pkl"
with open(metadata_file, "wb") as f:
    pickle.dump(metadata, f)
print(f"Metadata saved to {metadata_file}")

# -----------------------------
# Step 10: Optional CSV with combined_text
# -----------------------------
csv_out = "master_dataset_with_combined_text.csv"
df.to_csv(csv_out, index=False)
print(f"CSV saved as {csv_out}")

# -----------------------------
# Step 11: Download files
# -----------------------------
files.download(faiss_index_file)
files.download(metadata_file)
files.download(csv_out)
print("FAISS index, metadata, and CSV are ready for download!")

